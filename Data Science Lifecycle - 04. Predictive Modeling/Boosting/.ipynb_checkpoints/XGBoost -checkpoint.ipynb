{
 "cells": [
  {
   "attachments": {
    "XGBdata.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f82630e2",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "\n",
    "XGBoost works with the concepts of boosting, where each model will build sequentially. Each model takes the previous model’s feedback and tries to have a laser view on the misclassification performed by the previous model. \n",
    "\n",
    "This feedback of building sequential models happens in parallel. Which helps in getting the XGBoost the fast it needs.\n",
    "\n",
    "\n",
    "### A Loss Function To Be Optimized\n",
    "- The selected loss function relies on the sort of problem which can be solved, and it must be differentiable. However, the numerous standard loss functions are supported, and you can set your preference. \n",
    "\n",
    "- For instance, classification problems might work with logarithmic loss, while regression problems may use a squared error. These differences are well explained in the article difference between R-Squared and Adjusted R-Squared.\n",
    "\n",
    "- An advantage of the gradient boosting technique is that another boosting algorithm does not need to be determined for every loss function that might need to be utilized. \n",
    "\n",
    "\n",
    "### When To Use XGBoost\n",
    " - You have a large number of training samples. The definition of large in this criterion varies. Generally, a dataset greater than 1000 training samples and a few features, maybe 100, is considered fair. \n",
    "\n",
    " - In practice, if the number of features in the training set is smaller than the number of training samples, XGBoost would work fine. \n",
    "\n",
    " - XGBoost works when you have a mixture of categorical and numeric features - Or just numeric features in the dataset. ‘\n",
    "\n",
    "### When To Not Use XGBoost\n",
    "- The XGBoost algorithm would not perform well when the dataset's problem is not suited for its features. \n",
    "More precisely, XGBoost would not work with a dataset with issues such as Natural Language Processing (NLP), computer vision, image recognition, and understanding problems. \n",
    "\n",
    "- These datasets are best solved with deep learning techniques.\n",
    "XGBoost should not be used when the size of the training dataset is small. If the training set is less than the number of features, XGBoost would not be efficient. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## XGBoost Algorithm\n",
    "\n",
    "\n",
    "### Step 1 -\n",
    "\n",
    "- Consider the E.g. of Credit Card Dataset, We calculate the probability of Target(Using loss function optimization) and depending upon the probability the Target Residuals are found, here as it is binary classification problem probability is assumed to be 0.5.\n",
    "\n",
    "- Probability is not the output of Base model, but the base model is constructed based on residuals for which we can calculate (probability or Average first).\n",
    "\n",
    "### Step 2 - \n",
    "\n",
    "- Decision tree in XGBoost is only Binary tree irrespective of any number of classes present in categorical data.\n",
    "\n",
    "- The feature that gives higher gain is selected first, If we have more than two categories and we know that we can just make binary tree then we try every combination of binary classification upon different classes of same feature and the one who gives best gain is selected.\n",
    "for e.g. there are three categories in Credit score feature then we try binary classification as (GB,N)(GN,b) and which ever gives better gain is selected again similar process is carried out for different features and the feature which gives better Gain is selected at Root. \n",
    "\n",
    "- Let's consider Salary feature for which we classify data by two categories i.e. <50, >50. Here if we had more than two categories then we would have tried every combination of binary classification on that perticular feature and the one who gives higher similarity score is selected.\n",
    "\n",
    "- Gain is calculated by Similarity weight and Similarity weight is calculated at every Node as follows\n",
    "\n",
    "Similarity Weight $= \\frac{\\sum_{i}^{n} (Residual)^2}{\\sum_{i}^{n} Probability(1-(Probability)+ \\lambda)}$\n",
    "\n",
    "        - Where,\n",
    "         1. Residual are the Residuals that are in Perticular Node\n",
    "         2. n is Number of Residual\n",
    "         3. probability is .......\n",
    "\n",
    "Similarity Weight for **Node1** that gives Salary <50\n",
    "\n",
    "S.W = $\\frac{[(-0.5)+0.5+0.5+(-0.5)]^2}{0.5(1-0.5)+0.5(1-0.5)+0.5(1-0.5)+0.5(1-0.5) + \\lambda}$\n",
    "\n",
    "S.W = 0\n",
    "\n",
    "Similary for **Root Node** and **Node2** is calculated as 0.14, 0.33 respectively\n",
    "\n",
    "Gain $= (\\sum S.W. of Nodes)-(S.W. of Root Node)$\n",
    "\n",
    "Gain for Salary feature with two categories >50 & <50 is..\n",
    "\n",
    "Gain $= (0+0.33)-(0.14) $\n",
    "\n",
    "Gain $= 0.21 $\n",
    "\n",
    "![XGBdata.png](attachment:XGBdata.png)"
   ]
  },
  {
   "attachments": {
    "XGB.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9c4bbd8f",
   "metadata": {},
   "source": [
    "![XGB.png](attachment:XGB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e6f03",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3 -\n",
    "\n",
    "- Once we get first split now another Feature is selected with higher gain and various binary combinations are tried at each Leaf Node and one that gives Better Gain is selected.\n",
    "\n",
    "- Here we select another feature **Credit** and we try every combination (GB, N)(GN,B) at every Leaf Node and one which gives higher gain is selected.\n",
    "\n",
    "### Step 4 - \n",
    "\n",
    "- The process of Splitting and formimg new Branches is continued until the condition of cover value is reached i.e. When we find Gain of Any branch if the Gain is less than cover value then prunning is done which causes splitting to get terminated.\n",
    "\n",
    "- Cover value is given by **Cover Value = Probability(1-(Probability)** if Gain is less than this then branching and splitting is stopped which is called as preprunning. \n",
    "\n",
    "\n",
    "### Step 5 -\n",
    "\n",
    "- Once the Tree is completed it is called as Base Model and It is Used to get new Probability and based on that new Residuals for complete dataset\n",
    "\n",
    "- Again the new Tree is Created in a similar way on New Probabilities and Residuals(unlike first probability new Probabilities may have different values for different records.)\n",
    "\n",
    "\n",
    "for first data entry \n",
    "Salary is <50 and Credit is B then it is Classifed to LeafNode1 then Similarity Weight of LeafNode1 is taken into consideration, then Base model Output finded for the given data by following formula.\n",
    "\n",
    "\n",
    "\n",
    "#### New Probability = $ \\sigma (Base Model Output + Learning Rate * Similarity Weight)$\n",
    "      \n",
    "      - Base Model Output = $log\\frac{Probability of data}{1 - Probability of data}\n",
    "        which is 0 in this case. \n",
    "      \n",
    "      - Learning Rate ($\\alpha $) is specified initially Suppose say 0.1\n",
    "      \n",
    "      - Similarity weight is the S.W. of Leaf Node to which given data is classified suppose say first Leaf Node which have S.W. as 1\n",
    "      \n",
    "New Probability = $\\sigma (0+(0.1* 1)$ = $\\sigma (0.1)$ = $\\frac {1}{1+e^-0.1}$\n",
    "               \n",
    "               = 0.6  this is the new Probability for first record\n",
    "\n",
    "\n",
    "#### New residual  = Target Value - Probability \n",
    "\n",
    "              = 0 - 0.6 \n",
    "              \n",
    "              = 0.4 \n",
    "              \n",
    "Similary this is calculated for every record and new Probabilirties and Residuals updated and used to create new Tree.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e36e2",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "- It can handle all kinds of irregularities.\n",
    "\n",
    "- It's easy to implement,but hard to tune hyperparameters because of many hyperparameters.\n",
    "\n",
    " \n",
    "\n",
    "### The XGBoost Advantage\n",
    "\n",
    "1. Regularization:\n",
    "\n",
    "Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
    "In fact, XGBoost is also known as a ‘regularized boosting‘ technique.\n",
    "\n",
    "2. Parallel Processing:\n",
    "\n",
    "XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "But hang on, we know that boosting is a sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from.\n",
    "\n",
    "3. XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "4. High Flexibility\n",
    "\n",
    "XGBoost allows users to define custom optimization objectives and evaluation criteria.\n",
    "\n",
    "5. Handling Missing Values\n",
    "XGBoost has an in-built routine to handle missing values.\n",
    "The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "\n",
    "6. Tree Pruning:\n",
    "A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "\n",
    "7. Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "\n",
    "8. Built-in Cross-Validation\n",
    "XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "\n",
    "9. Continue on Existing Model\n",
    "User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
    "GBM implementation of sklearn also has this feature so they are even on this point.\n",
    "\n",
    "\n",
    "# **Table of Contents** <a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "\n",
    "- 1 [What are hyperparameters](#1)\n",
    "- 2 [XGBoost hyperparameters](#2)\n",
    "   - 2.1 [General Parameters](#2.1)\n",
    "      - 2.1.1 [booster](#2.1.1)\n",
    "      - 2.1.2 [verbosity](#2.1.2)\n",
    "      - 2.1.3 [nthread](#2.1.3)\n",
    "   - 2.2 [Booster Parameters](#2.2)\n",
    "      - 2.2.1 [eta](#2.2.1)\n",
    "      - 2.2.2 [gamma](#2.2.2)\n",
    "      - 2.2.3 [max_depth](#2.2.3)\n",
    "      - 2.2.4 [min_child_weight](#2.2.4)\n",
    "      - 2.2.5 [max_delta_step](#2.2.5)\n",
    "      - 2.2.6 [subsample](#2.2.6)\n",
    "      - 2.2.7 [colsample_bytree, colsample_bylevel, colsample_bynode](#2.2.7) \n",
    "      - 2.2.8 [lambda](#2.2.8)\n",
    "      - 2.2.9 [alpha](#2.2.9)\n",
    "      - 2.2.10 [tree_method](#2.2.10)\n",
    "      - 2.2.11 [scale_pos_weight](#2.2.11)\n",
    "      - 2.2.12 [max_leaves](#2.2.12)\n",
    "   - 2.3 [Learning Task Parameters](#2.3)\n",
    "      - 2.3.1 [objective](#2.3.1)\n",
    "      - 2.3.2 [eval_metric](#2.3.2)\n",
    "      - 2.3.3 [seed](#2.3.3)\n",
    "- 3 [Basic Setup](#3)\n",
    "   - 3.1 [Import libraries](#3.1)\n",
    "   - 3.2 [Read dataset](#3.2)\n",
    "   - 3.3 [Declare feature vector and target variable](#3.3)\n",
    "   - 3.4 [Split data into separate training and test set](#3.4)\n",
    "- 4 [Bayesian Optimization with HYPEROPT](#4)\n",
    "   - 4.1 [What is HYPEROPT](#4.1)\n",
    "   - 4.2 [4 Parts of Optimization Process](#4.2)\n",
    "   - 4.3 [Bayesian Optimization Implementation](#4.3)\n",
    "      - 4.3.1 [Initialize domain space for range of values](#4.3.1)\n",
    "      - 4.3.2 [Define objective function](#4.3.2)\n",
    "      - 4.3.3 [Optimization algorithm](#4.3.3)\n",
    "      - 4.3.4 [Print Results](#4.3.4)\n",
    "- 5 [Results and Conclusion](#5)\n",
    "- 6 [References](#6)\n",
    "\n",
    "\n",
    "#  XGBoost hyperparameters <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- Generally, the XGBoost hyperparameters have been divided into 4 categories. They are as follows -\n",
    "\n",
    "  - 1. General parameters\n",
    "  - 2. Booster parameters\n",
    "  - 3. Learning task parameters\n",
    "  - 4. Command line parameters\n",
    " \n",
    "- Before running a XGBoost model, we must set three types of parameters - **general parameters**, **booster parameters** and **task parameters**.\n",
    "\n",
    "- The fourth type of parameters are **command line parameters**. They are only used in the console version of XGBoost. So, we will skip these parameters and limit our discussion to the first three type of parameters.\n",
    "\n",
    "## 1 General Parameters <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- These parameters guide the overall functioning of the XGBoost model. \n",
    "\n",
    "- In this section, we will discuss three hyperparameters - **booster**, **verbosity** and **nthread**.\n",
    "\n",
    "- Please visit [XGBoost General Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters) for detailed discussion on general parameters.\n",
    "\n",
    "\n",
    "### **1.1 booster** <a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **booster[default = gbtree]**\n",
    "\n",
    "   - **booster** parameter helps us to choose which booster to use.\n",
    "   - It helps us to select the type of model to run at each iteration. \n",
    "   - It has 3 options - **gbtree**, **gblinear** or **dart**.\n",
    "   \n",
    "       - **gbtree** and **dart** - use tree-based models, while\n",
    "       - **gblinear** uses linear models. \n",
    "   \n",
    "\n",
    "### **1.2 verbosity** <a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **verbosity[default = 1]**\n",
    "\n",
    "    - Verbosity of printing messages. \n",
    "    - Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).\n",
    "    \n",
    "    \n",
    "### **1.3 nthread** <a class=\"anchor\" id=\"2.1.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **nthread [default = maximum number of threads available if not set]**\n",
    "\n",
    "   - This is number of parallel threads used to run XGBoost.\n",
    "   - This is used for parallel processing and number of cores in the system should be entered.\n",
    "   - If you wish to run on all cores, value should not be entered and algorithm will detect automatically.\n",
    "   \n",
    "   \n",
    "There are other general parameters like **disable_default_eval_metric [default=0]**, **num_pbuffer [set automatically by XGBoost, no need to be set by user]** and **num_feature [set automatically by XGBoost, no need to be set by user]**.\n",
    "\n",
    "So, these parameters are taken care by XGBoost algorithm itself. Hence,we will not discuss these further.\n",
    "\n",
    "\n",
    "## **2 Booster Parameters** <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- We have 2 types of boosters - **tree booster** and **linear booster**.\n",
    "- We will limit our discussion to **tree booster** because it always outperforms the **linear booster** and thus the later is rarely used.\n",
    "- Please visit, [Parameters for Tree Booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster), for detailed discussion on booster parameters.\n",
    "\n",
    "\n",
    "### **2.1 eta** <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **eta [default=0.3, alias: learning_rate]**\n",
    "\n",
    "  - It is analogous to learning rate in GBM.\n",
    "  - It is the step size shrinkage used in update to prevent overfitting. \n",
    "  - After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "  - It makes the model more robust by shrinking the weights on each step.\n",
    "  - range : [0,1]\n",
    "  - Typical final values : 0.01-0.2.\n",
    "  \n",
    "  \n",
    "  \n",
    "### **2.2 gamma** <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **gamma [default=0, alias: min_split_loss]**\n",
    "\n",
    "   - A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "   - Gamma specifies the minimum loss reduction required to make a split.\n",
    "   - It makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "   - The larger gamma is, the more conservative the algorithm will be.\n",
    "   - Range: [0,∞]\n",
    "\n",
    "\n",
    "### **2.3 max_depth** <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **max_depth [default=6]**\n",
    "\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - It is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Increasing this value will make the model more complex and more likely to overfit. \n",
    "    - The value 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth. \n",
    "    - We should be careful when setting large value of max_depth because XGBoost aggressively consumes memory when training a deep tree.\n",
    "    - range: [0,∞] (0 is only accepted in lossguided growing policy when tree_method is set as hist.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "    \n",
    "    \n",
    "### **2.4 min_child_weight** <a class=\"anchor\" id=\"2.2.4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **min_child_weight [default=1]**\n",
    "\n",
    "   - It defines the minimum sum of weights of all observations required in a child.\n",
    "   - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "   - It is used to control over-fitting. \n",
    "   - Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "   - Too high values can lead to under-fitting. \n",
    "   - Hence, it should be tuned using CV.\n",
    "   - The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "   - range: [0,∞]\n",
    "   \n",
    "### **2.5 max_delta_step** <a class=\"anchor\" id=\"2.2.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **max_delta_step [default=0]**\n",
    "\n",
    "   - In maximum delta step we allow each tree’s weight estimation to be. \n",
    "   - If the value is set to 0, it means there is no constraint. \n",
    "   - If it is set to a positive value, it can help making the update step more conservative.\n",
    "   - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "   - Set it to value of 1-10 might help control the update.\n",
    "   - range: [0,∞]\n",
    "\n",
    "\n",
    "### **2.6 subsample** <a class=\"anchor\" id=\"2.2.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **subsample [default=1]**\n",
    "\n",
    "   - It denotes the fraction of observations to be randomly samples for each tree.\n",
    "   - Subsample ratio of the training instances. \n",
    "   - Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees.      - This will prevent overfitting. \n",
    "   - Subsampling will occur once in every boosting iteration.\n",
    "   - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "   - Typical values: 0.5-1\n",
    "   - range: (0,1]\n",
    "   \n",
    "   \n",
    "### **2.7 colsample_bytree, colsample_bylevel, colsample_bynode** <a class=\"anchor\" id=\"2.2.7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n",
    "\n",
    "   - This is a family of parameters for subsampling of columns.\n",
    "\n",
    "   - All **colsample_by** parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n",
    "\n",
    "   - **colsample_bytree** is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "\n",
    "   - **colsample_bylevel** is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n",
    "\n",
    "   - **colsample_bynode** is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n",
    "\n",
    "   - **colsample_by*** parameters work cumulatively. For instance, the combination **{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}** with 64 features will leave 8 features to choose from at each split.\n",
    "\n",
    "### **2.8 lambda** <a class=\"anchor\" id=\"2.2.8\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **lambda [default=1, alias: reg_lambda]**\n",
    "\n",
    "    - L2 regularization term on weights  (analogous to Ridge regression).\n",
    "    - This is used to handle the regularization part of XGBoost. \n",
    "    - Increasing this value will make model more conservative.\n",
    "    \n",
    "\n",
    "### **2.9 alpha** <a class=\"anchor\" id=\"2.2.9\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **alpha [default=0, alias: reg_alpha]**\n",
    "\n",
    "    - L1 regularization term on weights (analogous to Lasso regression).\n",
    "    - It can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n",
    "    - Increasing this value will make model more conservative.\n",
    "    \n",
    "    \n",
    "### **2.10 tree_method** <a class=\"anchor\" id=\"2.2.10\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **tree_method string [default= auto]**\n",
    "\n",
    "   - The tree construction algorithm used in XGBoost. \n",
    "\n",
    "   - XGBoost supports `approx`, `hist` and `gpu_hist` for distributed training. Experimental support for external memory is available for `approx` and `gpu_hist`.\n",
    "\n",
    "   - Choices: `auto`, `exact`, `approx`, `hist`, `gpu_hist`\n",
    "\n",
    "      - **auto**: Use heuristic to choose the fastest method.\n",
    "         - For small to medium dataset, exact greedy (exact) will be used.\n",
    "\n",
    "         - For very large dataset, approximate algorithm (approx) will be chosen.\n",
    "\n",
    "         - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice.\n",
    "\n",
    "     - **exact**: Exact greedy algorithm.\n",
    "\n",
    "     - **approx**: Approximate greedy algorithm using quantile sketch and gradient histogram.\n",
    "\n",
    "     - **hist**: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.\n",
    "\n",
    "     - **gpu_hist**: GPU implementation of hist algorithm.\n",
    "     \n",
    "     \n",
    "### **2.11 scale_pos_weight** <a class=\"anchor\" id=\"2.2.11\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **scale_pos_weight [default=1]**\n",
    "\n",
    "     - It controls the balance of positive and negative weights, \n",
    "     - It is useful for imbalanced classes. \n",
    "     - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "     - A typical value to consider: `sum(negative instances) / sum(positive instances)`.\n",
    "\n",
    "### **2.12 max_leaves** <a class=\"anchor\" id=\"2.2.11\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **max_leaves [default=0]**\n",
    "\n",
    "  - Maximum number of nodes to be added. \n",
    "  - Only relevant when `grow_policy=lossguide` is set.\n",
    "  \n",
    "- There are other hyperparameters like `sketch_eps`,`updater`, `refresh_leaf`, `process_type`, `grow_policy`, `max_bin`, `predictor` and `num_parallel_tree`.\n",
    "\n",
    "- For detailed discussion of these hyperparameters, please visit [Parameters for Tree Booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "\n",
    "## **3 Learning Task Parameters** <a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- They are used to specify the learning task and the corresponding learning objective. The objective options are below:\n",
    "\n",
    "### **3.1 objective** <a class=\"anchor\" id=\"2.3.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "\n",
    "- **objective [default=reg:squarederror]**\n",
    "\n",
    "- It defines the loss function to be minimized. Most commonly used values are given below -\n",
    "\n",
    "     - **reg:squarederror** : regression with squared loss.\n",
    "     \n",
    "     - **reg:squaredlogerror**: regression with squared log loss 1/2[log(pred+1)−log(label+1)]2.               - All input labels are required to be greater than -1. \n",
    "     \n",
    "     - **reg:logistic** : logistic regression\n",
    "     \n",
    "     - **binary:logistic** : logistic regression for binary classification, output probability\n",
    "     \n",
    "     - **binary:logitraw**: logistic regression for binary classification, output score before logistic transformation\n",
    "\n",
    "     - **binary:hinge** : hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n",
    "     \n",
    "     - **multi:softmax** : set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n",
    "\n",
    "     - **multi:softprob** : same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.                            \n",
    "                             \n",
    "### **3.2 eval_metric** <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **eval_metric [default according to objective]**\n",
    "\n",
    "\n",
    "- The metric to be used for validation data.\n",
    "- The default values are **rmse for regression**, **error for classification** and **mean average precision for ranking**.\n",
    "- We can add multiple evaluation metrics.\n",
    "- Python users must pass the metrices as list of parameters pairs instead of map.\n",
    "- The most common values are given below -\n",
    "\n",
    "   - **rmse** : [root mean square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
    "   - **mae** : [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "   - **logloss** : [negative log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood)\n",
    "   - **error** : Binary classification error rate (0.5 threshold).  It is calculated as `#(wrong cases)/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n",
    "   - **merror** : Multiclass classification error rate. It is calculated as `#(wrong cases)/#(all cases)`.\n",
    "   - **mlogloss** : [Multiclass logloss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n",
    "   - **auc**: [Area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve)\n",
    "   - **aucpr** : [Area under the PR curve](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "\n",
    "### **3.3 seed** <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- **seed [default=0]**\n",
    "\n",
    "  - The random number seed.\n",
    "  - This parameter is ignored in R package, use set.seed() instead.\n",
    "  - It can be used for generating reproducible results and also for parameter tuning.\n",
    "  \n",
    "  \n",
    "## General Approach for Parameter Tuning\n",
    "We will use an approach similar to that of GBM here. The various steps to be performed are:\n",
    "\n",
    "1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "\n",
    "2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "\n",
    "3. Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "\n",
    "4. Lower the learning rate and decide the optimal parameters .\n",
    "\n",
    "### Tuning\n",
    "- learning_rate: 0.01\n",
    "- n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low\n",
    "- max_depth: 3\n",
    "- subsample: 0.8\n",
    "- colsample_bytree: 1\n",
    "- gamma: 1\n",
    "\n",
    "2. Run model.fit(eval_set, eval_metric) and diagnose your first run, specifically the n_estimators parameter\n",
    "\n",
    "3. Optimize `max_depth` parameter. It represents the depth of each tree, which is the maximum number of different features used in each tree. I recommend going from a low max_depth (3 for instance) and then increasing it incrementally by 1, and stopping when there’s no performance gain of increasing it. This will help simplify your model and avoid overfitting\n",
    "\n",
    "4. Now play around with the learning rate and the features that avoids overfitting:\n",
    "\n",
    "- `learning_rate`: usually between 0.1 and 0.01. If you’re focused on performance and have time in front of you, decrease incrementally the learning rate while increasing the number of trees.\n",
    "\n",
    "- `subsample`, which is for each tree the % of rows taken to build the tree. I recommend not taking out too many rows, as performance will drop a lot. Take values from 0.8 to 1.\n",
    "\n",
    "- `colsample_bytree`: number of columns used by each tree. In order to avoid some columns to take too much credit for the prediction (think of it like in recommender systems when you recommend the most purchased products and forget about the long tail), take out a good proportion of columns. Values from 0.3 to 0.8 if you have many columns (especially if you did one-hot encoding), or 0.8 to 1 if you only have a few columns.\n",
    "\n",
    "- `gamma`: usually misunderstood parameter, it acts as a regularization parameter. Either 0, 1 or 5.\n",
    "And you’re good to go!\n",
    "\n",
    "The parameter base_score didn’t give me anything. Either it’s not relevant for convergence, or I don’t know how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5d8fd",
   "metadata": {},
   "source": [
    "## 1. Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a45a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac43b1",
   "metadata": {},
   "source": [
    "## 2. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc671b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.722360e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.598370</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.638140e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>3.232740e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          objid          ra       dec         u         g         r         i  \\\n",
       "0  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1  1.237650e+18  183.598370  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "\n",
       "          z  run  rerun  camcol  field     specobjid   class  redshift  plate  \\\n",
       "0  15.22531  752    301       4    267  3.722360e+18    STAR -0.000009   3306   \n",
       "1  16.39150  752    301       4    267  3.638140e+17    STAR -0.000055    323   \n",
       "2  16.80125  752    301       4    268  3.232740e+17  GALAXY  0.123111    287   \n",
       "3  15.90438  752    301       4    269  3.722370e+18    STAR -0.000111   3306   \n",
       "4  16.61326  752    301       4    269  3.722370e+18    STAR  0.000590   3306   \n",
       "\n",
       "     mjd  fiberid  \n",
       "0  54922      491  \n",
       "1  51615      541  \n",
       "2  52023      513  \n",
       "3  54922      510  \n",
       "4  54922      512  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/Shubham/Documents/Data Science/Notebooks/Data Store/Skyserver.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319be037",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19213908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>175.529987</td>\n",
       "      <td>14.836148</td>\n",
       "      <td>18.619355</td>\n",
       "      <td>17.371931</td>\n",
       "      <td>16.840963</td>\n",
       "      <td>16.583579</td>\n",
       "      <td>16.422833</td>\n",
       "      <td>981.034800</td>\n",
       "      <td>301.0</td>\n",
       "      <td>3.648700</td>\n",
       "      <td>302.380100</td>\n",
       "      <td>1.645022e+18</td>\n",
       "      <td>0.143726</td>\n",
       "      <td>1460.986400</td>\n",
       "      <td>52943.533300</td>\n",
       "      <td>353.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>47.783439</td>\n",
       "      <td>25.212207</td>\n",
       "      <td>0.828656</td>\n",
       "      <td>0.945457</td>\n",
       "      <td>1.067764</td>\n",
       "      <td>1.141805</td>\n",
       "      <td>1.203188</td>\n",
       "      <td>273.305024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666183</td>\n",
       "      <td>162.577763</td>\n",
       "      <td>2.013998e+18</td>\n",
       "      <td>0.388774</td>\n",
       "      <td>1788.778371</td>\n",
       "      <td>1511.150651</td>\n",
       "      <td>206.298149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>8.235100</td>\n",
       "      <td>-5.382632</td>\n",
       "      <td>12.988970</td>\n",
       "      <td>12.799550</td>\n",
       "      <td>12.431600</td>\n",
       "      <td>11.947210</td>\n",
       "      <td>11.610410</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.995780e+17</td>\n",
       "      <td>-0.004136</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>51578.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>157.370946</td>\n",
       "      <td>-0.539035</td>\n",
       "      <td>18.178035</td>\n",
       "      <td>16.815100</td>\n",
       "      <td>16.173333</td>\n",
       "      <td>15.853705</td>\n",
       "      <td>15.618285</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>3.389248e+17</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>51900.000000</td>\n",
       "      <td>186.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>180.394514</td>\n",
       "      <td>0.404166</td>\n",
       "      <td>18.853095</td>\n",
       "      <td>17.495135</td>\n",
       "      <td>16.858770</td>\n",
       "      <td>16.554985</td>\n",
       "      <td>16.389945</td>\n",
       "      <td>756.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>4.966580e+17</td>\n",
       "      <td>0.042591</td>\n",
       "      <td>441.000000</td>\n",
       "      <td>51997.000000</td>\n",
       "      <td>351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>201.547279</td>\n",
       "      <td>35.649397</td>\n",
       "      <td>19.259232</td>\n",
       "      <td>18.010145</td>\n",
       "      <td>17.512675</td>\n",
       "      <td>17.258550</td>\n",
       "      <td>17.141447</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>2.881300e+18</td>\n",
       "      <td>0.092579</td>\n",
       "      <td>2559.000000</td>\n",
       "      <td>54468.000000</td>\n",
       "      <td>510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>260.884382</td>\n",
       "      <td>68.542265</td>\n",
       "      <td>19.599900</td>\n",
       "      <td>19.918970</td>\n",
       "      <td>24.802040</td>\n",
       "      <td>28.179630</td>\n",
       "      <td>22.833060</td>\n",
       "      <td>1412.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>9.468830e+18</td>\n",
       "      <td>5.353854</td>\n",
       "      <td>8410.000000</td>\n",
       "      <td>57481.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              objid            ra           dec             u             g  \\\n",
       "count  1.000000e+04  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   1.237650e+18    175.529987     14.836148     18.619355     17.371931   \n",
       "std    0.000000e+00     47.783439     25.212207      0.828656      0.945457   \n",
       "min    1.237650e+18      8.235100     -5.382632     12.988970     12.799550   \n",
       "25%    1.237650e+18    157.370946     -0.539035     18.178035     16.815100   \n",
       "50%    1.237650e+18    180.394514      0.404166     18.853095     17.495135   \n",
       "75%    1.237650e+18    201.547279     35.649397     19.259232     18.010145   \n",
       "max    1.237650e+18    260.884382     68.542265     19.599900     19.918970   \n",
       "\n",
       "                  r             i             z           run    rerun  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.0   \n",
       "mean      16.840963     16.583579     16.422833    981.034800    301.0   \n",
       "std        1.067764      1.141805      1.203188    273.305024      0.0   \n",
       "min       12.431600     11.947210     11.610410    308.000000    301.0   \n",
       "25%       16.173333     15.853705     15.618285    752.000000    301.0   \n",
       "50%       16.858770     16.554985     16.389945    756.000000    301.0   \n",
       "75%       17.512675     17.258550     17.141447   1331.000000    301.0   \n",
       "max       24.802040     28.179630     22.833060   1412.000000    301.0   \n",
       "\n",
       "             camcol         field     specobjid      redshift         plate  \\\n",
       "count  10000.000000  10000.000000  1.000000e+04  10000.000000  10000.000000   \n",
       "mean       3.648700    302.380100  1.645022e+18      0.143726   1460.986400   \n",
       "std        1.666183    162.577763  2.013998e+18      0.388774   1788.778371   \n",
       "min        1.000000     11.000000  2.995780e+17     -0.004136    266.000000   \n",
       "25%        2.000000    184.000000  3.389248e+17      0.000081    301.000000   \n",
       "50%        4.000000    299.000000  4.966580e+17      0.042591    441.000000   \n",
       "75%        5.000000    414.000000  2.881300e+18      0.092579   2559.000000   \n",
       "max        6.000000    768.000000  9.468830e+18      5.353854   8410.000000   \n",
       "\n",
       "                mjd       fiberid  \n",
       "count  10000.000000  10000.000000  \n",
       "mean   52943.533300    353.069400  \n",
       "std     1511.150651    206.298149  \n",
       "min    51578.000000      1.000000  \n",
       "25%    51900.000000    186.750000  \n",
       "50%    51997.000000    351.000000  \n",
       "75%    54468.000000    510.000000  \n",
       "max    57481.000000   1000.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b133ac",
   "metadata": {},
   "source": [
    "### It is a Multiclas classification as their are Three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b64fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GALAXY    4998\n",
       "STAR      4152\n",
       "QSO        850\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54340c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   objid      10000 non-null  float64\n",
      " 1   ra         10000 non-null  float64\n",
      " 2   dec        10000 non-null  float64\n",
      " 3   u          10000 non-null  float64\n",
      " 4   g          10000 non-null  float64\n",
      " 5   r          10000 non-null  float64\n",
      " 6   i          10000 non-null  float64\n",
      " 7   z          10000 non-null  float64\n",
      " 8   run        10000 non-null  int64  \n",
      " 9   rerun      10000 non-null  int64  \n",
      " 10  camcol     10000 non-null  int64  \n",
      " 11  field      10000 non-null  int64  \n",
      " 12  specobjid  10000 non-null  float64\n",
      " 13  class      10000 non-null  object \n",
      " 14  redshift   10000 non-null  float64\n",
      " 15  plate      10000 non-null  int64  \n",
      " 16  mjd        10000 non-null  int64  \n",
      " 17  fiberid    10000 non-null  int64  \n",
      "dtypes: float64(10), int64(7), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75a1a9",
   "metadata": {},
   "source": [
    "#### Null value is not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be66fcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(data.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59469ab",
   "metadata": {},
   "source": [
    "### Dropping unnecessary features.\n",
    "\n",
    "- **objid** and **specobjid** are just identifiers for accessing the rows back when they were stored in the original databank. Therefore we will not need them for classification as they are not related to the outcome.\n",
    "\n",
    "- The features **run**, **rerun**, **camcol** and **field** are values which describe parts of the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d097551",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['objid', 'run', 'rerun', 'camcol', 'field', 'specobjid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f808d",
   "metadata": {},
   "source": [
    "## 4. Splitting into Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c1ba2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ra        dec         u         g         r         i         z  \\\n",
      "0     183.531326   0.089693  19.47406  17.04240  15.94699  15.50342  15.22531   \n",
      "1     183.598370   0.135285  18.66280  17.21449  16.67637  16.48922  16.39150   \n",
      "2     183.680207   0.126185  19.38298  18.19169  17.47428  17.08732  16.80125   \n",
      "3     183.870529   0.049911  17.76536  16.60272  16.16116  15.98233  15.90438   \n",
      "4     183.883288   0.102557  17.55025  16.26342  16.43869  16.55492  16.61326   \n",
      "...          ...        ...       ...       ...       ...       ...       ...   \n",
      "9995  131.316413  51.539547  18.81777  17.47053  16.91508  16.68305  16.50570   \n",
      "9996  131.306083  51.671341  18.27255  17.43849  17.07692  16.71661  16.69897   \n",
      "9997  131.552562  51.666986  18.75818  17.77784  17.51872  17.43302  17.42048   \n",
      "9998  131.477151  51.753068  18.88287  17.91068  17.53152  17.36284  17.13988   \n",
      "9999  131.665012  51.805307  19.27586  17.37829  16.30542  15.83548  15.50588   \n",
      "\n",
      "      redshift  plate    mjd  fiberid  \n",
      "0    -0.000009   3306  54922      491  \n",
      "1    -0.000055    323  51615      541  \n",
      "2     0.123111    287  52023      513  \n",
      "3    -0.000111   3306  54922      510  \n",
      "4     0.000590   3306  54922      512  \n",
      "...        ...    ...    ...      ...  \n",
      "9995  0.027583    447  51877      246  \n",
      "9996  0.117772    447  51877      228  \n",
      "9997 -0.000402   7303  57013      622  \n",
      "9998  0.014019    447  51877      229  \n",
      "9999  0.118417    447  51877      233  \n",
      "\n",
      "[10000 rows x 11 columns]\n",
      "0         STAR\n",
      "1         STAR\n",
      "2       GALAXY\n",
      "3         STAR\n",
      "4         STAR\n",
      "         ...  \n",
      "9995    GALAXY\n",
      "9996    GALAXY\n",
      "9997      STAR\n",
      "9998    GALAXY\n",
      "9999    GALAXY\n",
      "Name: class, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = data.drop(\"class\", axis=1)\n",
    "y = data[\"class\"]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b32d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ff680",
   "metadata": {},
   "source": [
    "## 5. XGBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc64186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf9115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      GALAXY       0.99      0.99      0.99       973\n",
      "         QSO       0.97      0.98      0.97       172\n",
      "        STAR       1.00      1.00      1.00       855\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14268b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[968,   5,   0],\n",
       "       [  4, 168,   0],\n",
       "       [  2,   0, 853]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d3ae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
